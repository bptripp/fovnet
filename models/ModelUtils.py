#Run as Python 3.7!
import os

import collections
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.special import logsumexp
import pickle
from PIL import Image
import gzip

from skimage import io, transform
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils, datasets

from numpy.fft  import fft2, ifft2

import numba
from numba import jit

import cv2
from skimage.filters import gaussian




def make_dataset(dir, class_to_idx, center_points):
    """
    Returns a dictionary which maps an integer index to an image and its label (as a class index).
    Required for Dataloaders.
    Ensures we have center_points for all items we're going to train on. 
    
    Parameters:
        class_to_idx (dict): Dict with items (class_name, class_index). 
        Generated by Dataloaders by default
    """
    images = []
    dir = os.path.expanduser(dir)
    
    def is_valid_file(x):
        """
        Ensure we have saliencies for this image
        """
        x = x.split("CLS-LOC/")[1]
        return x in center_points
    
    for target in sorted(class_to_idx.keys()):
        d = os.path.join(dir, target)
        if not os.path.isdir(d):
            continue
        for root, _, fnames in sorted(os.walk(d)):
            for fname in sorted(fnames):
                path = os.path.join(root, fname)
                if is_valid_file(path):
                    item = (path, class_to_idx[target])
                    images.append(item)

    return images

def np_fftconvolve(A, B):
    """
    Convolution via Discrete Fourier Transform
    """
    return np.real(ifft2(fft2(A)*fft2(B, s=A.shape)))

def get_cropped_saliency(ldp, size=256):
    """
    Crop the edge of saliencies so there is zero (or very low) probability 
    around edges that have less than the tolerance amount of pixels within
    image.
    
    Parameters:
        ldp: Log Density Probability, a 2D saliency map
        size: The size of the image 
    """
    tolerance = 0.8
    
    A1 = np.ones(ldp.shape)
    B1 = np.ones((size, size))

    shape = np.add(A1.shape, B1.shape)
    A2 = np.zeros(shape)
    B2 = np.zeros(shape)

    A2[:A1.shape[0],:A1.shape[1]] = A1
    B2[:B1.shape[0],:B1.shape[1]] = B1

    temp = np_fftconvolve(B2,A2)[128:-128, 128:-128]

    new_ldp = ldp.copy()

    new_ldp[np.where(temp<size*size*tolerance)] = -1e+10

    if (new_ldp > -1e+10).any():
        return new_ldp
    else:
        return ldp
    
def makeGaussian(size, fwhm = 3, center=None):
    """ Make a square gaussian kernel.
    size is the length of a side of the square
    fwhm is full-width-half-maximum, which
    can be thought of as an effective radius.
    """

    x = np.arange(0, size[1], 1, float)
    y = np.arange(0, size[0], 1, float)
    y = y[:,np.newaxis]
    
    if center is None:
        x0 = y0 = size // 2
    else:
        x0 = center[1]
        y0 = center[0]
    
    return np.exp(-4*np.log(2) * ((x-x0)**2 + (y-y0)**2) / fwhm**2)

def get_focal_points(saliency_name, num_points):
    """
    Retun a tuple of focal points based on the top num_points most salient locations
    after subtracting a square gaussian from subsequent samples.
    
    Parameters:
        Saliency_name: str, file to read log density predictions
        num_points: int, number of focal points to gather
    """
    with gzip.GzipFile(saliency_name, 'rb') as f:
        ldp = np.load(f)      

    ldp = get_cropped_saliency(ldp)
    ldp = np.exp(ldp)
    
    centers = []
    for i in range(num_points):
        center = np.unravel_index(np.argmax(ldp),ldp.shape)
        centers.append(center)
        gaussian_cut = makeGaussian(ldp.shape, center=center, fwhm=60)
        ldp = (1-gaussian_cut)*ldp
    
    del ldp
    return centers;

def get_top_corner(center, padding, crop_size):
    """
    Return the top left corner of a crop given the center, padding, and crop size
    """
    return (center[0]+(padding-crop_size//2), center[1]+(padding-crop_size//2))

def get_image_range(center, crop_size=256):
    """
    Return the image range given a center and crop size as a tuple of ((x_low, x_high),(y_low, y_high))
    """
    
    #x, #y
    return ( (center[0]-(crop_size//2) , center[0]+(crop_size//2) ), \
        (center[1]-(crop_size//2) , center[1]+(crop_size//2) ) )

def get_padding(image, image_range):
    """
    Given an image range for an image, determine the amount of padding required
    """
    padding = 0
    if min(image_range[0]) < 0:
        padding = max(padding, abs(min(image_range[0])))
    if min(image_range[1]) < 0:
        padding = max(padding, abs(min(image_range[1])))

    width, height = image.size
    if max(image_range[0]) > width or max(image_range[1]) > height:
        padding = max(padding, 
                      max(image_range[0]) - width, 
                      max(image_range[1]) - height
                     )
    return int(padding)